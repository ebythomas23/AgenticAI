{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e76157c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786e602e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340e24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"]= os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] =os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Langsmith Tracking and Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]= os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] =os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa1c308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x107dc2900> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x107dc3380> root_client=<openai.OpenAI object at 0x107dc01a0> root_async_client=<openai.AsyncOpenAI object at 0x107dc30e0> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model= \"o1-mini\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8faad6db",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhat is langchain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agentic_ai_2.0/venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:378\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    368\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    373\u001b[39m     **kwargs: Any,\n\u001b[32m    374\u001b[39m ) -> BaseMessage:\n\u001b[32m    375\u001b[39m     config = ensure_config(config)\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    377\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    388\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agentic_ai_2.0/venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:963\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    955\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    956\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    960\u001b[39m     **kwargs: Any,\n\u001b[32m    961\u001b[39m ) -> LLMResult:\n\u001b[32m    962\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agentic_ai_2.0/venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:782\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    781\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m         )\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    790\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agentic_ai_2.0/venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1028\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1026\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1032\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agentic_ai_2.0/venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1130\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1128\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m   1129\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agentic_ai_2.0/venv/lib/python3.13/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agentic_ai_2.0/venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:1087\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1044\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1084\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1085\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1086\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1087\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agentic_ai_2.0/venv/lib/python3.13/site-packages/openai/_base_client.py:1256\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1244\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1251\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1252\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1253\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1254\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1255\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/agentic_ai_2.0/venv/lib/python3.13/site-packages/openai/_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "llm.invoke(\"what is langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31275e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, so I need to explain what LangChain is. Let me start by recalling what I know. I remember hearing that it\\'s related to AI and large language models. Maybe it\\'s a framework or a tool for developers? I think it helps in building applications that use LLMs.\\n\\nWait, the user is asking for a detailed explanation. I should structure this properly. Let me break it down. First, I should mention that LangChain is a framework. It\\'s probably used in the context of machine learning and natural language processing. The key points would be its purpose, features, and how it\\'s used.\\n\\nI remember reading that LangChain simplifies the process of creating applications that interact with large language models. So, its main goal is to make it easier for developers to work with these models without getting bogged down by the complexities. It might handle things like data retrieval, processing, and maybe even deployment.\\n\\nNow, what are the core components of LangChain? I think it includes modules for different parts of an application. There\\'s something about Chains, which are sequences of actions. Then Agents, which can make decisions and execute tasks. Tools might be parts of the system that Agents can use. Memory could be handling state between interactions. And Models, obviously, integrating various LLMs.\\n\\nI should also mention compatibility with different LLMs. OpenAI\\'s models come to mind, but maybe others like Cohere, Hugging Face, or Anthropic are supported too. That\\'s important because it gives developers flexibility.\\n\\nUse cases would be helpful to explain. Examples like chatbots, document analysis, task automation. These show how LangChain is applied in real-world scenarios. Also, maybe mention some of the key companies or projects using it, but I\\'m not sure of specific names.\\n\\nWait, am I mixing up any concepts here? Let me make sure I don\\'t confuse LangChain with other similar frameworks. LangChain is specifically focused on the architecture and components needed to build these AI applications, right? It\\'s not just a toolkit but a structured way to handle workflows involving LLMs.\\n\\nOh, and there\\'s something about the concept of \"chains\" which are workflows combining LLMs with other functions. Like, you can have a chain where the model processes data, interacts with a database, and then provides an answer. Agents might be part of these chains to decide what steps to take next.\\n\\nI should also touch on why someone would use LangChain instead of working directly with an LLM\\'s API. Probably because it abstracts away a lot of the boilerplate code, allows for modular design, and provides best practices for handling things like context window management, data retrieval, and error handling.\\n\\nHmm, maybe mention the key concepts again: Chains, Agents, Tools, Models, and Memory. Explaining each briefly would help. Also, the fact that it\\'s open-source and community-driven might be a point to include. Oh, and it\\'s developed by a company called Llama Index before it became part of LangChain, or maybe that\\'s a different project? Wait, I think Llama Index and LangChain are both from the same team, maybe?\\n\\nWait, no, I think LangChain was created by Llama, but I\\'m not sure. Maybe I should check that, but since I can\\'t look it up, I\\'ll just stick to what I know. Focus on the technical aspects.\\n\\nAlso, important to highlight that LangChain is Python-based. The documentation is probably in Python, and that\\'s the main language for using it. It uses libraries like Pydantic for data validation, which is part of its structure.\\n\\nLet me outline the structure of the answer:\\n\\n1. Introduction: Define LangChain as a framework for developing AI applications with LLMs.\\n2. Core Components: Chains, Agents, Tools, Models, Memory.\\n3. Key Features: Modularity, Compatibility, Ecosystem, Scalability.\\n4. Use Cases: Examples like chatbots, document analysis, etc.\\n5. Why Use It: Simplifies workflows, reduces boilerplate, handles complexities.\\n6. Conclusion: Summarize its role in the AI development process.\\n\\nI need to ensure I don\\'t miss any critical parts. Maybe mention that it\\'s open source and actively maintained, which contributes to its reliability. Also, maybe some examples of projects or companies using it to ground the explanation.\\n\\nWait, I should also clarify that LangChain isn\\'t an LLM itself but a way to build applications that use LLMs. That\\'s an important distinction to avoid confusion.\\n\\nAdditionally, the concept of Agents being autonomous programs that can execute tasks by invoking tools and models. Chains link these components together. Memory allows for maintaining context across interactions, which is crucial for things like chatbots needing to remember previous messages.\\n\\nI should explain each component with a bit more detail. For instance, Chains can be simple or complex, combining multiple steps. Tools could be external APIs or internal functions. Models can be swapped in and out depending on the task, so the framework is flexible.\\n\\nOkay, putting it all together in a coherent way without getting too technical but still informative. Make sure to use examples to clarify abstract concepts. Avoid jargon where possible, but since it\\'s a technical question, some terms are necessary.\\n\\nI should also mention that LangChain provides pre-built modules that handle common tasks, so developers don\\'t have to start from scratch. This accelerates development and ensures best practices are followed.\\n\\nAnother point: LangChain helps in creating applications that are scalable. Since LLMs can be resource-heavy, the framework might help in optimizing how they\\'re used, like caching responses or managing API calls efficiently.\\n\\nWait, but does it do that explicitly? Maybe not in all cases, but the modular design allows for such optimizations. I might not want to overstate that without concrete details.\\n\\nOverall, the goal is to explain LangChain\\'s purpose, its main features, how it\\'s structured, and why it\\'s useful for developers working with LLMs. I need to make sure the explanation is clear and covers all essential aspects without unnecessary details.\\n</think>\\n\\n**LangChain** is an open-source Python framework designed to simplify the process of building applications that integrate and utilize **Large Language Models (LLMs)**, such as those from OpenAI, Anthropic, or Hugging Face. It provides a structured and modular approach to developing AI-driven tools, abstracting much of the complexity associated with working with LLMs directly. Here\\'s a detailed breakdown:\\n\\n---\\n\\n### **Core Concepts & Features**\\n1. **Chains**:\\n   - **Purpose**: Workflows composed of sequential steps (e.g., data processing, LLM interactions, and post-processing).\\n   - **Examples**: \\n     - Combining data retrieval (e.g., from a database) with an LLM to generate a response.\\n     - Creating \"multi-step\" logic (e.g., \"fetch data → analyze it → summarize it with an LLM → format the output\").\\n\\n2. **Agents**:\\n   - **Purpose**: Autonomous programs that decide which tools or steps to execute to achieve a goal.\\n   - **How It Works**: Agents analyze inputs, select relevant tools (e.g., web search APIs, LLMs, or custom functions), and execute them to solve a problem. They can even iterate on tasks if needed.\\n\\n3. **Tools**:\\n   - **Purpose**: External functionalities that an Agent can call (e.g., APIs, databases, or custom code).\\n   - **Examples**: \\n     - Web search APIs (e.g., Google, SerpAPI).\\n     - Database query tools.\\n     - Custom tools for specific tasks (e.g., PDF processing).\\n\\n4. **Models**:\\n   - **Purpose**: Integrates various LLMs (e.g., OpenAI’s GPT, Anthropic’s Claude) and other models (e.g., embedding models for vector databases).\\n   - **Flexibility**: Easily switch between models or combine them in workflows.\\n\\n5. **Memory**:\\n   - **Purpose**: Maintains context across interactions (e.g., in chatbots).\\n   - **Types**: \\n     - **Contextual Memory**: Stores conversation history or temporary data.\\n     - **External Memory**: Connects to databases or vector stores (e.g., ChromaDB, Pinecone).\\n\\n---\\n\\n### **Key Features**\\n- **Modular Architecture**: Build applications by composing pre-built components (Chains, Agents, Tools) rather than starting from scratch.\\n- **Model Agnosticism**: Works with multiple LLM providers, making it easy to experiment with different models or switch vendors.\\n- **Ecosystem**: Integrates with tools like vector databases (e.g., Chroma, FAISS), vector search (e.g., Pinecone), and chat interfaces (e.g., Streamlit, Gradio).\\n- **Scalability**: Simplifies creating complex workflows (e.g., multi-step reasoning) while managing resources efficiently.\\n\\n---\\n\\n### **Common Use Cases**\\n1. **Chatbots & Conversational AI**:\\n   - Enable chatbots to handle multi-turn conversations, maintain context, and integrate external data (e.g., a support chatbot that looks up customer records).\\n2. **Document Analysis**:\\n   - Link LLMs with document processing tools to summarize, query, or analyze text (e.g., extracting insights from legal contracts).\\n3. **Automation**:\\n   - Automate tasks like email summarization, code generation, or data entry by combining LLMs with APIs (e.g., automating social media posts using a schedule).\\n4. **Custom Workflows**:\\n   - Create custom decision-making logic (e.g., an Agent that checks weather data and suggests activities).\\n\\n---\\n\\n### **Why Use LangChain?**\\n- **Reduces Boilerplate**: Focus on logic, not infrastructure. LangChain handles API calls, error handling, and data formatting.\\n- **Accelerates Development**: Pre-built templates and tools cut down development time.\\n- **Extensibility**: Easily extend functionality with custom Chains, Agents, or Tools.\\n- **Best Practices**: Built-in patterns for common challenges like context window management and data retrieval.\\n\\n---\\n\\n### **Example Workflow**\\nImagine a **customer service chatbot**:\\n1. **User Input**: \"What’s my account balance?\"\\n2. **Agent**: Analyzes the query, determines needed data (account details).\\n3. **Tool Invocation**: Calls a database API to fetch the balance.\\n4. **LLM Integration**: Uses an LLM to format the response naturally (e.g., \"Your balance is $100. Would you like to check recent transactions?\").\\n5. **Memory**: Stores the conversation history to provide continuity in subsequent interactions.\\n\\n---\\n\\n### **Who Uses It?**\\nLangChain is widely adopted in:\\n- Startups and enterprises building AI tools.\\n- Researchers prototyping LLM applications.\\n- Developers needing to integrate LLMs into existing systems (e.g., content generation, data analysis).\\n\\n---\\n\\n### **Getting Started**\\n- **Installation**: `pip install langchain`.\\n- **Example Code Snippet**:\\n  ```python\\n  from langchain import OpenAI, LLMMathChain\\n\\n  llm = OpenAI(temperature=0.9)\\n  math_chain = LLMMathChain.from_llm(llm)\\n  answer = math_chain.run(\"What is 2 + 2?\")\\n  print(answer)  # Output: \"4\"\\n  ```\\n  This simple example shows how LangChain wraps LLM functionality for common tasks.\\n\\n---\\n\\n### **Conclusion**\\nLangChain is a foundational tool for developers working with LLMs, offering a structured, scalable approach to build AI applications. By abstracting complexity and providing modular components, it bridges the gap between raw LLM APIs and real-world use cases like chatbots, data analysis, and automation. Its open-source nature and active community ensure it stays adaptable to evolving AI tools and best practices.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 257, 'prompt_tokens': 2185, 'total_tokens': 2442, 'completion_time': 0.591140043, 'prompt_time': 0.12990372, 'queue_time': 0.089155596, 'total_time': 0.721043763}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_ea798e78f0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--1fdd8c7a-d417-4a16-af55-8ec78494f934-0', usage_metadata={'input_tokens': 2185, 'output_tokens': 257, 'total_tokens': 2442})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model =ChatGroq(model=\"qwen-qwq-32b\")\n",
    "model.invoke(\"what is langchain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fad9d92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for **Large Language Model**.\n",
      "\n",
      "It's a type of artificial intelligence (AI) that excels at understanding and generating human-like text.  \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "**Large:** These models are trained on massive datasets of text and code, often consisting of billions or even trillions of words. This vast amount of data allows them to learn complex patterns and relationships in language.\n",
      "\n",
      "**Language:** LLMs are specifically designed to process and generate text. They can understand the nuances of language, including grammar, syntax, and semantics.\n",
      "\n",
      "**Model:**  An LLM is a mathematical representation of language. It uses algorithms to analyze and predict the likelihood of certain words appearing together in a given context.\n",
      "\n",
      "**What can LLMs do?**\n",
      "\n",
      "* **Generate creative content:** Write stories, poems, articles, and even code.\n",
      "* **Answer questions:** Provide informative and comprehensive answers to a wide range of questions.\n",
      "* **Summarize text:** Condense large amounts of text into concise summaries.\n",
      "* **Translate languages:** Accurately translate text from one language to another.\n",
      "* **Chat and converse:** Engage in natural-sounding conversations with humans.\n",
      "\n",
      "**Examples of LLMs:**\n",
      "\n",
      "* GPT-3 (Generative Pre-trained Transformer 3)\n",
      "* LaMDA (Language Model for Dialogue Applications)\n",
      "* BERT (Bidirectional Encoder Representations from Transformers)\n",
      "* BLOOM (BigScience Large Open-science Open-access Multilingual Language Model)\n",
      "\n",
      "**Keep in mind:**\n",
      "\n",
      "While LLMs are impressive, they are still under development. They can sometimes generate inaccurate or biased information, and they lack real-world understanding and common sense.\n",
      "\n",
      "Let me know if you have any other questions about LLMs!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results =model.invoke(\"what is llm\")\n",
    "print(results.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7baf65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a expert Ai engineer . Provide answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### prompt engineering \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a expert Ai engineer . Provide answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7c052a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x1200016d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x1200020d0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model =ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4773fa44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a expert Ai engineer . Provide answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x1200016d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x1200020d0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### chaining\n",
    "\n",
    "chain = prompt | model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beb6f84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI engineer, I can definitely tell you about Langsmith! \n",
      "\n",
      "Langsmith is an open-source tool built by the folks at **Replicate**. It's designed to make it **easier to build and share AI applications**, particularly those involving large language models (LLMs). \n",
      "\n",
      "Here's a breakdown of its key features and what makes it special:\n",
      "\n",
      "**1. Streamlined LLM Development:**\n",
      "\n",
      "* **Simplified Prompting:** Langsmith provides a user-friendly interface for crafting and testing prompts for LLMs. It helps you experiment with different phrasing and parameters to get the best results.\n",
      "* **Pipeline Building:** You can chain together multiple LLMs and other tools (like search engines or APIs) to create complex AI workflows. Imagine building a chatbot that not only understands your questions but also retrieves information from the web to provide comprehensive answers.\n",
      "* **Data Management:** Langsmith helps you manage and process the data your AI applications need. It supports various data formats and offers tools for cleaning, transforming, and preparing data for training or inference.\n",
      "\n",
      "**2. Collaboration and Sharing:**\n",
      "\n",
      "* **Open-Source Nature:** Being open-source means Langsmith's code is freely accessible, allowing anyone to contribute to its development or adapt it to their specific needs.\n",
      "* **Replicate Integration:** Langsmith seamlessly integrates with Replicate, a platform for sharing and running AI models. This makes it easy to deploy your Langsmith-built applications and share them with others.\n",
      "\n",
      "**3. Accessibility:**\n",
      "\n",
      "* **User-Friendly:** Langsmith is designed to be accessible to both experienced AI developers and those just starting out. Its intuitive interface and helpful documentation make it easier to get started.\n",
      "* **Cloud-Based:** You can use Langsmith in the cloud, eliminating the need for local hardware setup and allowing you to scale your projects as needed.\n",
      "\n",
      "**In essence, Langsmith empowers you to:**\n",
      "\n",
      "* **Build sophisticated AI applications quickly and efficiently.**\n",
      "* **Collaborate with others on AI projects.**\n",
      "* **Share your AI creations with the world.**\n",
      "\n",
      "If you're interested in exploring the world of LLM development, Langsmith is definitely worth checking out!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"input\":\" can you tell me something about langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5418a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me tell you about LangChain! As an AI engineer, I find it a tremendously powerful tool.\n",
      "\n",
      "**What is LangChain?**\n",
      "\n",
      "LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). Think of it as a toolbox brimming with components that let you build sophisticated AI applications, going far beyond simply asking an LLM a question and getting a response.\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **Chain Creation:** LangChain's core strength lies in its ability to \"chain\" together different LLMs, tools, and other components. This allows you to create complex workflows where an LLM interacts with external data sources, performs calculations, or even controls other software.\n",
      "\n",
      "* **Memory:**  It provides mechanisms to give LLMs a memory, allowing them to retain context from previous interactions in a conversation. This is crucial for building chatbots or applications that require understanding a history of events.\n",
      "\n",
      "* **Agents:** LangChain lets you build AI agents. These are autonomous entities that can reason, plan, and take actions in the world based on their interactions with LLMs and other tools.\n",
      "\n",
      "* **Prompt Templates:** It offers a library of prompt templates to structure your interactions with LLMs more effectively. These templates can be customized to suit your specific needs, making it easier to get the desired output from the model.\n",
      "\n",
      "* **Integration with Tools:** LangChain seamlessly integrates with a wide range of tools, including search engines, databases, APIs, and more. This allows your LLMs to access and process external information, expanding their capabilities significantly.\n",
      "\n",
      "**Use Cases:**\n",
      "\n",
      "LangChain's versatility opens doors to countless applications:\n",
      "\n",
      "* **Chatbots:** Create more sophisticated chatbots that can remember past conversations, access relevant information, and provide personalized responses.\n",
      "* **Question Answering Systems:** Build systems that can answer complex questions by querying multiple knowledge sources and synthesizing information.\n",
      "* **Text Summarization and Generation:** Automate the summarization of lengthy documents or generate creative content based on given prompts.\n",
      "* **Code Generation:** Assist developers by generating code snippets in different programming languages.\n",
      "\n",
      "* **Data Analysis:** Leverage LLMs to analyze and interpret data, uncovering insights and patterns.\n",
      "\n",
      "**Getting Started:**\n",
      "\n",
      "LangChain has excellent documentation and a friendly community. You can find tutorials, examples, and support online to help you get started.\n",
      "\n",
      "\n",
      "Let me know if you have any more specific questions about LangChain. I'm happy to dive deeper into particular aspects or use cases!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser= StrOutputParser()\n",
    "chain = prompt|model|output_parser\n",
    "\n",
    "response = chain.invoke({\"input\":\" can you tell me about langchain\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47243a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "output_parser = JsonOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b88db940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "output_parser = JsonOutputParser()\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"answer the user query \\n {format_instruction} \\n q={query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b23188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'langsmith': {'description': 'Langsmith is an open-source framework for developing and deploying large language models (LLMs).', 'features': ['Modular design allows for customization and extensibility.', 'Supports various LLM architectures (e.g., Transformer, T5).', 'Provides tools for training, fine-tuning, and evaluating LLMs.', 'Offers a user-friendly interface for interacting with LLMs.', 'Promotes collaboration and knowledge sharing within the AI community.'], 'website': 'https://github.com/langsmithai/langsmith', 'community': 'Active community on GitHub and Discord'}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|model|output_parser\n",
    "\n",
    "response = chain.invoke({\"query\":\"can you tell something about langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20cf6007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert ai engineer . provide  the response in json. provide answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "output_parser = JsonOutputParser()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert ai engineer . provide  the response in json. provide answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1aa0cd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'Langsmith is an open-source platform developed by the LAION team that simplifies the process of fine-tuning large language models (LLMs). It offers a user-friendly interface and a comprehensive set of tools to help users customize LLMs for specific tasks and domains.\\n\\nHere are some key features of Langsmith:\\n\\n* **Streamlined Fine-Tuning:** Langsmith provides a simplified workflow for fine-tuning LLMs, making it accessible to a wider range of users.\\n* **Pre-Trained Models:** It offers access to a collection of pre-trained LLMs, allowing users to start with a strong foundation.\\n* **Dataset Management:** Langsmith includes tools for managing and preparing datasets for fine-tuning.\\n* **Experiment Tracking:** It allows users to track and compare the performance of different fine-tuning experiments.\\n* **Community Support:** Langsmith has an active community of developers and users who contribute to its development and provide support.\\n\\n**Benefits of Using Langsmith:**\\n\\n* **Customization:** Fine-tune LLMs to meet specific needs and improve performance on target tasks.\\n* **Accessibility:** Makes LLM fine-tuning more accessible to users with limited technical expertise.\\n* **Efficiency:** Streamlines the fine-tuning process, saving time and resources.\\n* **Transparency:** Provides insights into the fine-tuning process and the resulting model performance.\\n\\n**Overall, Langsmith is a valuable tool for anyone interested in exploring and leveraging the power of large language models.'}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|model|output_parser\n",
    "response = chain.invoke({\"input\":\"can you tell me about langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1ee4c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define  data structure\n",
    "class Joke(BaseModel):\n",
    "    setup : str = Field(description=\"question to set up the joke\")\n",
    "    punchline:str = Field( description=\"answer to resolve the joke\")\n",
    "\n",
    "model = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \" tell me a joke\"\n",
    "    \n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "        template=\"answer the user query \\n {format_instructions}\\n {query}\",\n",
    "        input_variables=[\"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    \n",
    "    )\n",
    "\n",
    "\n",
    "chain = prompt|model|parser\n",
    "chain.invoke({\"query\":\"joke_query\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b037f1",
   "metadata": {},
   "source": [
    "### Assisgment:\n",
    "Create a simple assistant that uses any LLM and should be pydantic, when we ask about any product it should give you two information product Name, product details tentative price in USD (integer). use chat Prompt Template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7f9bc3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Samsung Galaxy S24' detail='The Samsung Galaxy S24 is a highly anticipated smartphone expected to be released in early 2024. It is rumored to feature a cutting-edge design, a powerful processor, and a stunning display. ' price=799.99\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import  ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "#initialize the llm \n",
    "model = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "# define the pydantic model\n",
    "class Product(BaseModel):\n",
    "    name:str = Field(description=\"name of the product\")\n",
    "    detail:str= Field(description=\"information about the product\")\n",
    "    price:float= Field(description=\"price of product in usd \")\n",
    "\n",
    "# setup output parser\n",
    "parser =PydanticOutputParser(pydantic_object=Product)\n",
    "\n",
    "#prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful Assitant that provide  the product information\")\n",
    "       , (\"user\",\" Provide details about the following product :{input} \\n {format_instructions}\")\n",
    "        \n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "chain = prompt| model | parser\n",
    "\n",
    "response = chain.invoke({\"input\":\" samsung galazy s24\",\"format_instructions\":parser.get_format_instructions()})\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
